# Tensor Analysis

## Introduction

Tensors are a generalization of scalars and vectors.

- scalars - tensors with rank 0
- vectors - tensors with rank 1

A given tensor with rank 'n' has the following properties:

- it has components labelled by n indices, with each index
  having value from 1 to d (dimension of the tensor). Thus $d^n$
  components are possible?
- the components transfer in a specified manner on coordinate
  transformation.

## Covariant and contravariant transformations

If we transform a vector $A = A_1 \hat e_1 + A_2 \hat e_2 + A_3 \hat e_3$
into another coordinate system with vector as
$A' = A_1' \hat e_1' + A_2' \hat e_2' + A_3' \hat e_3'$
we can write the relation between both at some $A_i$ index as.

$$ A_i' = \sum_j (\hat e_i' \cdot \hat e_j) A_j $$

Since the $\hat e_i$ and the $\hat e_j$ are linearly related and using
chain rule we get (doubt 1: they compare partial derivates with direction
cosines).

$$ A_i' = \sum_j \frac{\partial x_i'}{\partial x_j} A_j $$

But if you take a look at gradient of a scalar an observer how they
transform, we have:

<body>
$$ (\nabla \varphi)_i' = \frac{\partial \varphi}{\partial x_i'}  $$
$$ (\nabla \varphi)_i' = \sum_j \frac{\partial x_j}{\partial x_i'} \frac{\partial
\varphi}{\partial x_j} $$
</body>

If we define both $A$ and $\nable \varphi$ as vectors then the transformation
properties will be different, thus they might be of different types of vectors.

Thus $A$ here is called contravariant vector and we denote by writing indexes in
superscript, meanwhile we have $\nable \varphi$ covariant vectors and we
denote them by writing indexes in subscript.

$$ (A')^i = \sum_j \frac{\partial (x')^i}{\partial x^j} A^j $$
$$ A_i' = \sum_j \frac{\partial x^j}{\partial (x')^i} A_j $$

Here the cartesian coordinates are also contravariant thus (actually it doesn't
really matter). But position vector is by definition contravariant $x^i$.
(doubt 2: why)

### Einstein's convention

If we have an index in both upper and lower index and they cancel out, that
means there is a summation (we don't need to write summation because its waste
of space).

### Rank of tensor

The the number of parital derivates in product is equal to the rank. For example

<body>
$$ (A')_{ij} = \frac{\partial x^l}{\partial (x')^i} \frac{\partial x^k}{\partial (x')^j} A_{kl} $$
$$ (B')_i^j = \frac{\partial (x')^j}{\partial x^k} \frac{\partial x^l}{\partial (x')^i} A^k_l $$
</body>

The transformation laws are independent of the frame of reference and physically
relevant properties.

In 3-d space for example the second rank tensor will have $3^2 = 9$ components
represented by:

$$
A = \begin{pmatrix}
    A^{11} & A^{12} & A^{13} \\
    A^{21} & A^{22} & A^{23} \\
    A^{31} & A^{32} & A^{33}
\end{pmatrix}
$$

In this way we can also view the above equations as matrix equations, here's and
example:

<body>
$$ (A')^{ij} = \sum_{kl} S_{ik}A^{kl} (S^T)_{lj} $$
</body>
kinda like 
$$ A' = SAS^T$$

Where the $S_{ik} = \frac{\partial x^i}{\partial x^k}$ meanwhile $(S^T)_{lj} =
\frac{\partial x^j}{\partial x^l}$

### Operations on tensors

Operations on tensors are like operating on a matrix, but can be generalized to
any rank. We can only add tensors of similar form (mixed, contravariant and
covariant)

## Definition

Finally the definition of tensors is as follows;

> Tensors are systems of components organized by one or more indices that
> transform according to specific rules under a set of transformations. The
> number of indices is called the rank of the tensor.

This means tensors have no meaning without the transformation rules

## Symmetry of tensors

Just like symmetric and antisymmetric matrices, there are tensors.

### Isotropic tensors

2nd rank isotropic tensor has a better name called Kronecker delta, which is
symmetric across both diagonals (its actually a unit matrix)

It is a mixed tensor written as
$$ \delta^i_j = \frac{\partial (x')^i}{\partial x^k} \frac{\partial x^l}{\partial (x')^j} \delta^k_l $$

But in this particular tensor, if k=l then it is 1 else it is zero. Thus the
summation can be expanded by eliminating the l dependence as
$$ \delta^i_j = \frac{\partial (x')^i}{\partial x^k} \frac{\partial x^k}{\partial (x')^j} $$
$$ \delta^i_j = \frac{\partial (x')^i}{\partial (x')^j} $$

Since both unit vectors are independent of each other, the derivative is 0.
(except when i =j )

### Contraction

This is a generalization of dot product, where two indices, one covariant and
other contravariant are set equal to each other. The rank reduces by 2.

We can contract a mixed second order matrix by setting i=j:
$$ (B')^i_j = \delta^i_j B^k_l = B^k_k (scalar) $$
See how the B converts into B' using summation convention.

## Direct product

> The direct product is a technique for creating new, higher-rank tensors.

Its just the product of the two matrices but genearlized, any type of product is
allowed, they indexes just add up to each other, eg:

$$ C^{ij}_{klm} = A^i_k B^j_{lm} $$

It is commonly used to get higher order tensors from lower order.
Eg:

<body>
$$ (C')^j_i = (a')_i(b')^j = \frac{\partial x^k}{\partial (x')_i}
a_k \frac{\partial (x')^j}{\partial x^l} b_l
= \frac{\partial x^k}{\partial (x')_i}
\frac{\partial (x')^j}{\partial x^l} C^l_k$$
</body>

### Inverse transformation

In cartisian coordinates the partial in partial derivative has no meaning,
meaning because the other variables are constant with respect to the current
ones anyways.
$$ \frac{\partial x^i}{d(x')^j} = \frac{d(x')^j}{\partial x^i}$$

But this doesn't hold for non cartesian system here is an example:

$$ \frac{\partial y(x,z)}{\partial a} = \frac{\partial a(z)}{\partial y}$$

Here the x,y are kept constant and y is being differentiated, but the right hand
side the z is being kept constant.

Thus we need do define an inverse transformation such that:
$$ (A')^j = \frac{\partial (x')^j}{\partial x^i} A^i $$
$$ A^i = \frac{\partial x^i}{\partial (x')^j} (A')^j $$

### Quotient rule

If there was an equation and both sides have tensors for different form, then we
introduce an unknown quanity which is there to fix the form (like dimentional
constants).

$$ m_i = P E^j $$

dipole moment is a covariant vector and Electric field is contravariant, thus we
introduct P, (full form is $P_{ij}$), which is called the quotient.

> Quotient rule is a substituite for illegal division in tensors.

### Pseudo tensors

(doubt 3)
When rotations are improper, then the sign factor comes in and sometimes the
tensor we found is negetive of what we want. Thus we define pseudotensors in
such a place and have a sign rule
$$ T \otimes T = P \otimes P = T$$
$$ T \otimes P = T \otimes P = P$$
