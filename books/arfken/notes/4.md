# Tensor Analysis

## Introduction

Tensors are a generalization of scalars and vectors.

- scalars - tensors with rank 0
- vectors - tensors with rank 1

A given tensor with rank 'n' has the following properties:

- it has components labelled by n indices, with each index
  having value from 1 to d (dimension of the tensor). Thus $d^n$
  components are possible?
- the components transfer in a specified manner on coordinate
  transformation.

## Covariant and contravariant transformations

If we transform a vector $A = A_1 \hat e_1 + A_2 \hat e_2 + A_3 \hat e_3$
into another coordinate system with vector as
$A' = A_1' \hat e_1' + A_2' \hat e_2' + A_3' \hat e_3'$
we can write the relation between both at some $A_i$ index as.

$$ A_i' = \sum_j (\hat e_i' \cdot \hat e_j) A_j $$

Since the $\hat e_i$ and the $\hat e_j$ are linearly related and using
chain rule we get

$$ A_i' = \sum_j \frac{\partial x_i'}{\partial x_j} A_j $$

But if you take a look at gradient of a scalar an observer how they
transform, we have:

<body>
$$ (\nabla \varphi)_i' = \frac{\partial \varphi}{\partial x_i'}  $$
$$ (\nabla \varphi)_i' = \sum_j \frac{\partial x_j}{\partial x_i'} \frac{\partial
\varphi}{\partial x_j} $$
</body>

If we define both $A$ and $\nable \varphi$ as vectors then the transformation
properties will be different, thus they might be of different types of vectors.

Thus $A$ here is called contravariant vector and we denote by writing indexes in
superscript, meanwhile we have $\nable \varphi$ covariant vectors and we
denote them by writing indexes in subscript.

$$ (A')^i = \sum_j \frac{\partial (x')^i}{\partial x^j} A^j $$
$$ A_i' = \sum_j \frac{\partial x^j}{\partial (x')^i} A_j $$

Here the cartesian coordinates are also contravariant thus (actually it doesn't
really matter). But position vector is by definition contravariant $x^i$.
(doubt 2: why)

### Einstein's convention

If we have an index in both upper and lower index and they cancel out, that
means there is a summation (we don't need to write summation because its waste
of space).

### Rank of tensor

The the number of parital derivates in product is equal to the rank. For example

<body>
$$ (A')_{ij} = \frac{\partial x^l}{\partial (x')^i} \frac{\partial x^k}{\partial (x')^j} A_{kl} $$
$$ (B')_i^j = \frac{\partial (x')^j}{\partial x^k} \frac{\partial x^l}{\partial (x')^i} A^k_l $$
</body>

The transformation laws are independent of the frame of reference and physically
relevant properties.

In 3-d space for example the second rank tensor will have $3^2 = 9$ components
represented by:

$$
A = \begin{pmatrix}
    A^{11} & A^{12} & A^{13} \\
    A^{21} & A^{22} & A^{23} \\
    A^{31} & A^{32} & A^{33}
\end{pmatrix}
$$

In this way we can also view the above equations as matrix equations, here's and
example:

<body>
$$ (A')^{ij} = \sum_{kl} S_{ik}A^{kl} (S^T)_{lj} $$
</body>
kinda like 
$$ A' = SAS^T$$

Where the $S_{ik} = \frac{\partial x^i}{\partial x^k}$ meanwhile $(S^T)_{lj} =
\frac{\partial x^j}{\partial x^l}$

### Operations on tensors

Operations on tensors are like operating on a matrix, but can be generalized to
any rank. We can only add tensors of similar form (mixed, contravariant and
covariant)

## Definition

Finally the definition of tensors is as follows;

> Tensors are systems of components organized by one or more indices that
> transform according to specific rules under a set of transformations. The
> number of indices is called the rank of the tensor.

This means tensors have no meaning without the transformation rules

## Symmetry of tensors

Just like symmetric and antisymmetric matrices, there are tensors.

### Isotropic tensors

2nd rank isotropic tensor has a better name called Kronecker delta, which is
symmetric across both diagonals (its actually a unit matrix)

It is a mixed tensor written as
$$ \delta^i_j = \frac{\partial (x')^i}{\partial x^k} \frac{\partial x^l}{\partial (x')^j} \delta^k_l $$

But in this particular tensor, if k=l then it is 1 else it is zero. Thus the
summation can be expanded by eliminating the l dependence as
$$ \delta^i_j = \frac{\partial (x')^i}{\partial x^k} \frac{\partial x^k}{\partial (x')^j} $$
$$ \delta^i_j = \frac{\partial (x')^i}{\partial (x')^j} $$

Since both unit vectors are independent of each other, the derivative is 0.
(except when i =j )

### Contraction

This is a generalization of dot product, where two indices, one covariant and
other contravariant are set equal to each other. The rank reduces by 2.

We can contract a mixed second order matrix by setting i=j:
$$ (B')^i_j = \delta^i_j B^k_l = B^k_k (scalar) $$
See how the B converts into B' using summation convention.

## Direct product

> The direct product is a technique for creating new, higher-rank tensors.

Its just the product of the two matrices but genearlized, any type of product is
allowed, they indexes just add up to each other, eg:

$$ C^{ij}_{klm} = A^i_k B^j_{lm} $$

It is commonly used to get higher order tensors from lower order.
Eg:

<body>
$$ (C')^j_i = (a')_i(b')^j = \frac{\partial x^k}{\partial (x')_i}
a_k \frac{\partial (x')^j}{\partial x^l} b_l
= \frac{\partial x^k}{\partial (x')_i}
\frac{\partial (x')^j}{\partial x^l} C^l_k$$
</body>

### Inverse transformation

In cartisian coordinates the partial in partial derivative has no meaning,
meaning because the other variables are constant with respect to the current
ones anyways.
$$ \frac{\partial x^i}{d(x')^j} = \frac{d(x')^j}{\partial x^i}$$

But this doesn't hold for non cartesian system here is an example:

$$ \frac{\partial y(x,z)}{\partial a} = \frac{\partial a(z)}{\partial y}$$

Here the x,y are kept constant and y is being differentiated, but the right hand
side the z is being kept constant.

Thus we need do define an inverse transformation such that:
$$ (A')^j = \frac{\partial (x')^j}{\partial x^i} A^i $$
$$ A^i = \frac{\partial x^i}{\partial (x')^j} (A')^j $$

### Quotient rule

If there was an equation and both sides have tensors for different form, then we
introduce an unknown quanity which is there to fix the form (like dimentional
constants).

$$ m_i = P E^j $$

dipole moment is a covariant vector and Electric field is contravariant, thus we
introduct P, (full form is $P_{ij}$), which is called the quotient.

> Quotient rule is a substituite for illegal division in tensors.

### Pseudo tensors

(doubt 3)
When rotations are improper, then the sign factor comes in and sometimes the
tensor we found is negetive of what we want. Thus we define pseudotensors in
such a place and have a sign rule
$$ T \otimes T = P \otimes P = T$$
$$ T \otimes P = T \otimes P = P$$

### Metric tensors

- Basis vector - We can write in form of others using
  <body>
  $$
      \epsilon_i = \frac{\partial x}{\partial q^i}\hat e_x +
      \frac{\partial y}{\partial q^i}\hat e_y  +
      \frac{\partial z}{\partial q^i}\hat e_z
      $$
  </body>

  We define metric tensor as $g_{ij} = \epsilon_i \epsilon_j$, which can define
  displacement as follows
  <body>
  $$
  (ds)^2 = (\epsilon_i dq^i)\cdot(\epsilon_j dq^j)
  $$
  $$ (ds)^2 = g_{ij} dq^i dq^j $$
    </body>

Since this tensor is used to define displacement (magnitude without inversion,
rotation and all of that partial tensor bs), it is called metric tensor.

#### Contravariant tensors

<body>
Just like covariant tensors defined by:
$$
    \epsilon_i = \frac{\partial x}{\partial q^i}\hat e_x +
    \frac{\partial y}{\partial q^i}\hat e_y  +
    \frac{\partial z}{\partial q^i}\hat e_z
    $$
</body>

<body>
We can also define contravariant tensors like:
$$
    \epsilon^i = \frac{\partial q^i}{\partial x}\hat e_x +
    \frac{\partial q^i}{\partial y}\hat e_y  +
    \frac{\partial q^i}{\partial z}\hat e_z
    $$
</body>

### Covariant Derivatives

For keeping it simple let's study the contravariant derivates of a vector.

$$ (V')^i = \frac{\partial x^i}{\partial q_k} V^k $$

When we perform derivative we will have two terms

$$
\frac{\partial (V')^i}{\partial q^j} =  \frac{\partial x^i}{\partial q_k}
\frac{ \partial V^k}{\partial q_j} +
 \frac{\partial^2 x^i}{\partial q_k \partial q_j} V^k
$$

Using the basis vectors symbols

$$
\frac{\partial (V')^i}{\partial q^j} =
\frac{ \partial V^k}{\partial q_j}\epsilon_k  +
 \frac{\partial \epsilon_k}{\partial q_j} V^k
$$

We call the $\frac{\partial \epsilon_k}{\partial q_j}$ terms as Christoffel
symbols of the second kind.

<body>
$$ \frac{\partial \epsilon*k}{\partial q_j} = \Gamma^\mu_{jk} \epsilon_\mu $$
</body>

(The index $\mu$ is just arbitrary when taking sum can be converted)

Since basis vectors are orthogonal we can find out the value by taking its dot
product with contravariant basis.

<body>
$$ \Gamma^m_{jk} = \epsilon_m \cdot \frac{\partial \epsilon_k}{\partial q_j}  $$
</body>

Thus the equation can be now written as:

<body>
$$
\frac{\partial (V')^i}{\partial q^j} =
\left( \frac{ \partial V^k}{\partial q_j}  +
 \Gamma^k_{jk} V^k \right) \epsilon_k
$$
</body>

We call the term as covariant derivative of V.

<body>
$$ \frac{\partial V'}{\partial q^j} = V^k_{;j}\epsilon_k $$
</body>

Where

<body>
$$V^k_{;j} = \left( \frac{\partial V^k}{\partial q_j}+\Gamma^k_{jk} V^k \right)$$
</body>

### First Christoffel symbols

The first Christoffel symbols can be made using lowering the second Christoffel
symbols using metric tensor

<body>
$$ [ij, k] = g_{mk} \Gamma^m_{ij} $$
</body>

the first christoffel symbol can be found out using the basis vector similar to
other, but the covariant basis vector.

$$ [ij, k] = \epsilon_k \cdot \frac{\partial \epsilon_i}{\partial q^j} $$

But one cool observation is that, we can obtain the first Christoffel symbols
too from metric tensor by taking the derivative.

<body>
$$ \partial g_{ij} = \epsilon_i \cdot \epsilon_j $$
$$ \frac{\partial g_{ij}}{\partial q^k} =
 \frac{\partial \epsilon_j}{\partial q^k} \cdot \epsilon_i +
 \frac{\partial \epsilon_i}{\partial q^k} \cdot \epsilon_j
 $$
$$ \frac{\partial g_{ij}}{\partial q^k} = [ik, j] + [jk, i] $$
</body>

Thus we have

$$
\frac{1}{2}\left[ \frac{\partial g_{ik}}{\partial q^j} +
\frac{\partial g_{jk}}{\partial q^i} -
\frac{\partial g_{ij}}{\partial q^k}
 \right] = [ij, k]
$$

### Tensor derivative operators

#### Gradient

Its just derivative of the scalar at given index and give it the basis vector
direction
$$ \nabla \varphi = \frac{\partial \varphi}{\partial q^i} \epsilon^i$$

#### Divergence

Its like taking dot product between basis vector at an index times the
derivative of the contravariant representation of vector,

<body> 
$$ \nable \cdot V = V^i\epsilon_i $$ 
$$ \nabla \cdot V = \epsilon^j \cdot \frac{\partial(V^i\epsilon_i)}{\partial q^j} $$
$$ \nabla \cdot V = \epsilon^j V^i_{;j}\epsilon_i $$
$$ \nabla \cdot V = \epsilon^j \left( \frac{\partial V^i}{\partial q^j} + 
V^k \Gamma^i_{jk} \right)\epsilon_i $$
</body>

Using the fact that basis vectors are orthogonal:

<body> 
$$ \nabla \cdot V = \frac{\partial V^i}{\partial q^i} + V^k \Gamma^i_{ik}$$
</body>

The Christoffel factor given will simplify down to

<body> 
$$ \Gamma^i_{ik} = \frac{1}{2} g^{im}\frac{\partial g_{im}}{\partial q^k} $$
</body>

We can write above in terms of determinant

<body>
$$ \frac{d det(g)}{dq^k} = det(g)g^{im}\frac{\partial g\_{im}}{\partial q^k} $$
</body>

Thus,

<body>
$$
\Gamma^i_{ik} = \frac{1}{2det(g)} \frac{d det(g)}{d q^k}
= \frac{1}{[det(g)]^{\frac{1}{2}}} \frac{\partial
[det(g)]^{\frac{1}{2}}}{\partial q^k}
$$
</body>

Thus the divergence can now be given by applying product rule in reverse we get:

<body>
$$
\nabla \cdot V = V^i_{;i} = \frac{1}{[det(g)]^{\frac{1}{2}}}
\frac{\partial}{\partial q^k}
\left( [det(g)]^{1/2} V^k \right)
$$
</body>

#### Laplacian

Just substituite the V there with $\frac{\partial \phi}{\partial q^i}$

#### Curl

It is the sum of difference derivative of one component wrt other and
other wrt it.

$$
\frac{\partial V_i}{\partial q^j} - \frac{\partial V_j}{\partial q^i} =
V_{i;j} - V_{j;i}
$$
